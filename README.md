# gpt2-lite
GPT-2 reproduction from scratch. Network architecture, optimized training pipeline, hyperparameters from GPT-2/GPT-3 papers and full training run.







## Referenses

- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 paper
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3 paper
