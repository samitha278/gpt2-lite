{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWM7hJnG2aJURPi9KgWR5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt2-lite/blob/main/repro_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bPIqVkE4774D"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from transformers import GPT2LMHeadModel , pipeline , set_seed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "params_dict = model.state_dict()"
      ],
      "metadata": {
        "id": "wYxb4Ymb9G94"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_d = {}\n",
        "for k,v, in params_dict.items():\n",
        "   real_d[k] = v.shape"
      ],
      "metadata": {
        "id": "WCaGOmn4T-Ap"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (k, v) in enumerate(params_dict.items()):\n",
        "    if i < 20 or i > len(params_dict) - 5:\n",
        "        print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0cM_AfU_3cZ",
        "outputId": "43c76fe7-8d0a-41d3-e19f-2b3c2cff14ce"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(220064)\n",
        "gen = pipeline('text-generation' , model = 'gpt2')\n",
        "prompt = \"Who is the first president is sri lanka?\"\n",
        "gen(prompt,max_new_tokens= 10 , max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSVv2UGeBQMJ",
        "outputId": "8ac9d66c-b922-4474-c31a-16422b1b5b1c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=10) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Who is the first president is sri lanka? and jeeves! We are a huge country'}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "zFv7ianNKwc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([\n",
        "    [[1,2,3],\n",
        "    [4,5,6]],\n",
        "\n",
        "    [[7,8,9],\n",
        "    [12,11,10]],\n",
        "\n",
        "    ])\n",
        "\n",
        "print(temp@temp.transpose(-2,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHK6wiS5Jts",
        "outputId": "824241e3-93f0-4929-c00a-047aafa8c154"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 14,  32],\n",
            "         [ 32,  77]],\n",
            "\n",
            "        [[194, 262],\n",
            "         [262, 365]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "DU4dcOWtEJQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "  batch_size = 32\n",
        "  block_size = 8\n",
        "  n_head = 4\n",
        "  n_layer = 6\n",
        "  n_embd = 128"
      ],
      "metadata": {
        "id": "kdwoieXV320Y"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "uvhc_246MjZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "    self.key = nn.Linear(n_embd,head_size)\n",
        "    self.query = nn.Linear(n_embd,head_size)\n",
        "    self.value = nn.Linear(n_embd,head_size)\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    weight = (query @ key.transpose(-1,-2)  ) * (self.head_size**-0.5)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim = -1)\n",
        "\n",
        "    value = self.value(x)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "aEd3CGid6Nk3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    n_head = config.n_head\n",
        "    n_embd = config.n_embd\n",
        "\n",
        "\n",
        "    self.self_attns = nn.ModuleList([SelfAttentionHead(config)  for i in range(n_head)])\n",
        "    self.projection = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = torch.cat([sa(x) for sa in self.self_attns],dim=-1)\n",
        "    out = self.projection(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XQwRITlEkec"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = MultiHead(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wq2RceWBFB5",
        "outputId": "18701b1c-4ec5-49eb-ea0b-d02f69bb9285"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHead(\n",
            "  (self_attns): ModuleList(\n",
            "    (0-3): 4 x SelfAttentionHead(\n",
            "      (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (value): Linear(in_features=128, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (projection): Linear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "self_attns.0.tril torch.Size([8, 8])\n",
            "self_attns.0.key.weight torch.Size([32, 128])\n",
            "self_attns.0.key.bias torch.Size([32])\n",
            "self_attns.0.query.weight torch.Size([32, 128])\n",
            "self_attns.0.query.bias torch.Size([32])\n",
            "self_attns.0.value.weight torch.Size([32, 128])\n",
            "self_attns.0.value.bias torch.Size([32])\n",
            "self_attns.1.tril torch.Size([8, 8])\n",
            "self_attns.1.key.weight torch.Size([32, 128])\n",
            "self_attns.1.key.bias torch.Size([32])\n",
            "self_attns.1.query.weight torch.Size([32, 128])\n",
            "self_attns.1.query.bias torch.Size([32])\n",
            "self_attns.1.value.weight torch.Size([32, 128])\n",
            "self_attns.1.value.bias torch.Size([32])\n",
            "self_attns.2.tril torch.Size([8, 8])\n",
            "self_attns.2.key.weight torch.Size([32, 128])\n",
            "self_attns.2.key.bias torch.Size([32])\n",
            "self_attns.2.query.weight torch.Size([32, 128])\n",
            "self_attns.2.query.bias torch.Size([32])\n",
            "self_attns.2.value.weight torch.Size([32, 128])\n",
            "self_attns.2.value.bias torch.Size([32])\n",
            "self_attns.3.tril torch.Size([8, 8])\n",
            "self_attns.3.key.weight torch.Size([32, 128])\n",
            "self_attns.3.key.bias torch.Size([32])\n",
            "self_attns.3.query.weight torch.Size([32, 128])\n",
            "self_attns.3.query.bias torch.Size([32])\n",
            "self_attns.3.value.weight torch.Size([32, 128])\n",
            "self_attns.3.value.bias torch.Size([32])\n",
            "projection.weight torch.Size([128, 128])\n",
            "projection.bias torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa = SelfAttentionHead(config)\n",
        "dic = sa.state_dict()\n",
        "\n",
        "print(sa)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCWfjaogGpO5",
        "outputId": "4049ed6e-76bd-49ff-e6a7-3b2946d158f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttentionHead(\n",
            "  (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (value): Linear(in_features=128, out_features=32, bias=True)\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.weight torch.Size([32, 128])\n",
            "key.bias torch.Size([32])\n",
            "query.weight torch.Size([32, 128])\n",
            "query.bias torch.Size([32])\n",
            "value.weight torch.Size([32, 128])\n",
            "value.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "sa(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOL314zJHtCy",
        "outputId": "195d2437-7085-45ab-ad8a-8cd6a27f4dae"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03vJtScFGxc",
        "outputId": "c0dec490-5a7c-4681-b955-1d61df735b76"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Head class + Multi Head class -> Single class"
      ],
      "metadata": {
        "id": "Sk9w2QM73mus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "out = torch.randint(5,(32,4,4,3))\n",
        "out[0][0],out[0][1],out[0][2],out[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGJdSFYwU6z",
        "outputId": "6577e481-cf97-40fc-c33c-f0304925b9eb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0, 4, 0],\n",
              "         [0, 1, 1],\n",
              "         [0, 0, 4],\n",
              "         [4, 2, 4]]),\n",
              " tensor([[4, 0, 4],\n",
              "         [0, 2, 4],\n",
              "         [2, 2, 4],\n",
              "         [3, 2, 2]]),\n",
              " tensor([[0, 2, 4],\n",
              "         [2, 3, 3],\n",
              "         [0, 1, 4],\n",
              "         [1, 4, 4]]),\n",
              " tensor([[3, 1, 1],\n",
              "         [3, 1, 4],\n",
              "         [3, 4, 4],\n",
              "         [3, 0, 3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,nh,T,C = out.shape\n",
        "out = out.permute(0,2,1,3)\n",
        "out = out.reshape(B,T,nh*C)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZaLcAMJym6T",
        "outputId": "ec575d27-c893-45ae-ccaa-a10ca82db90a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 4, 0, 4, 0, 4, 0, 2, 4, 3, 1, 1],\n",
              "        [0, 1, 1, 0, 2, 4, 2, 3, 3, 3, 1, 4],\n",
              "        [0, 0, 4, 2, 2, 4, 0, 1, 4, 3, 4, 4],\n",
              "        [4, 2, 4, 3, 2, 2, 1, 4, 4, 3, 0, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = nn.ModuleList(nn.Linear(128,32) for _ in range(4))\n",
        "x = torch.randn((32,8,128))\n",
        "out1 = torch.stack([k(x) for k in key],dim=1)\n",
        "out2 = torch.stack([k(x) for k in key],dim=1)\n",
        "out = out1 @ out2.transpose(-1,-2)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFS9dObTXUo",
        "outputId": "a1928f6f-ae25-4463-b3b2-819153b14757"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "\n",
        "    self.key = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.query = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.value = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    key = torch.stack([k(x) for k in self.key],dim=1)\n",
        "    query = torch.stack([q(x) for q in self.query],dim=1)\n",
        "\n",
        "    weight = query @ key.transpose(-1,-2)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "    value = torch.stack([v(x) for v in self.value],dim=1)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    B,nh,T,C = out.shape\n",
        "    out = out.permute(0,2,1,3)\n",
        "    out = out.reshape(B,T,nh*C)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "IbRClTxwLGI7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = Attention(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGO-810DTkmz",
        "outputId": "a58b8fd1-d323-4aa6-d338-b82b3e68ecd9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention(\n",
            "  (key): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (query): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (value): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.0.weight torch.Size([32, 128])\n",
            "key.0.bias torch.Size([32])\n",
            "key.1.weight torch.Size([32, 128])\n",
            "key.1.bias torch.Size([32])\n",
            "key.2.weight torch.Size([32, 128])\n",
            "key.2.bias torch.Size([32])\n",
            "key.3.weight torch.Size([32, 128])\n",
            "key.3.bias torch.Size([32])\n",
            "query.0.weight torch.Size([32, 128])\n",
            "query.0.bias torch.Size([32])\n",
            "query.1.weight torch.Size([32, 128])\n",
            "query.1.bias torch.Size([32])\n",
            "query.2.weight torch.Size([32, 128])\n",
            "query.2.bias torch.Size([32])\n",
            "query.3.weight torch.Size([32, 128])\n",
            "query.3.bias torch.Size([32])\n",
            "value.0.weight torch.Size([32, 128])\n",
            "value.0.bias torch.Size([32])\n",
            "value.1.weight torch.Size([32, 128])\n",
            "value.1.bias torch.Size([32])\n",
            "value.2.weight torch.Size([32, 128])\n",
            "value.2.bias torch.Size([32])\n",
            "value.3.weight torch.Size([32, 128])\n",
            "value.3.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))"
      ],
      "metadata": {
        "id": "CinPQ96lYLSG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = h(x)"
      ],
      "metadata": {
        "id": "AyLUg3IyYYPv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZ0GiTfa2Uo",
        "outputId": "ec31e19e-d186-4fb2-d2ec-4ff408be3752"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention version"
      ],
      "metadata": {
        "id": "OYvO_Nk631cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        block_size = config.block_size\n",
        "\n",
        "        self.n_head = n_embd = config.n_head\n",
        "        self.n_embd = n_head = config.n_embd\n",
        "\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)     # fan out : n_head * 3 * head_size\n",
        "\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # C = n_embd = n_head * head_size\n",
        "\n",
        "        qkv = self.c_attn(x)    # B,T, 3*n_embd\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)    # each : B,T, n_head * head_size\n",
        "\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)         # B, n_head, T, T\n",
        "        att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v        # B, n_head, T, head_size\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)   # B, T , n_embd   (n_embd = n_head * head_size)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "o_BPwNCFy8Fs"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = Attention(config)\n",
        "# dic = h.state_dict()\n",
        "# print(h)\n",
        "# for k,v, in dic.items():\n",
        "#   print(k,v.shape)"
      ],
      "metadata": {
        "id": "e_ShacqE4YFK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "h(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUMU6hn34HQj",
        "outputId": "175fd038-6264-46bd-cbaa-7d0d7f6c6be0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch pretrained params from huggingface gpt-2 model"
      ],
      "metadata": {
        "id": "TYoo7YQULH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "@dataclass\n",
        "class GPT2Config:\n",
        "    block_size : int = 1024\n",
        "    vocab_size : int = 50257\n",
        "    n_layer : int = 12\n",
        "    n_head : int = 12\n",
        "    n_embd : int = 768\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size,config.n_embd),\n",
        "\n",
        "            h = nn.ModuleList([Block(config) for i in range(config.n_layer)]),\n",
        "\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd,config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x,targets= None):\n",
        "\n",
        "        B,T = x.shape\n",
        "        assert T<= self.config.block_size   # positional embd table max size = block_size\n",
        "        tx = self.transformer.wte(x)       #token embedding\n",
        "        px = self.transformer.wpe(torch.arange(0,T,self.config.block_size,device=device)) #positional embedding\n",
        "\n",
        "        x = tx+px     # add both\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits\n",
        "\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1,self.config.n_embd),targets.view(-1))\n",
        "            return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type='gpt2'):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        assert model_type == 'gpt2'\n",
        "\n",
        "        config_args = dict(n_layer=12, n_head=12, n_embd=768, vocab_size=50257, block_size=1024)\n",
        "        config = GPT2Config(**config_args)\n",
        "        model = GPT2(config)\n",
        "\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias')]\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith(('.attn.masked_bias', '.attn.bias'))]\n",
        "\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys)\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,idx,max_token):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.attn(self.ln_1(x)) + x\n",
        "        x = self.mlp(self.ln_2(x)) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.c_fc = nn.Linear(config.n_embd,4*config.n_embd)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        block_size = config.block_size\n",
        "\n",
        "        self.n_head = n_head = config.n_head\n",
        "        self.n_embd = n_embd = config.n_embd\n",
        "\n",
        "\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)     # fan out : n_head * 3 * head_size\n",
        "\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # C = n_embd = n_head * head_size\n",
        "\n",
        "        qkv = self.c_attn(x)    # B,T, 3*n_embd\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)    # each : B,T, n_head * head_size\n",
        "\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)         # B, n_head, T, T\n",
        "        att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v        # B, n_head, T, head_size\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)   # B, T , n_embd   (n_embd = n_head * head_size)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0PrZxQLYOLia"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "model = GPT2(config)\n",
        "dic = model.state_dict()\n",
        "\n",
        "\n",
        "new_d = {}\n",
        "for k,v, in dic.items():\n",
        "   new_d[k] = v.shape"
      ],
      "metadata": {
        "id": "TCvhKDb0LPGa"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (k, v) in enumerate(dic.items()):\n",
        "    if i < 20 or i > len(dic) - 5:\n",
        "        print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5uZcEI8eofm",
        "outputId": "6dcb50d9-4af1-4f24-b3c9-05eec4d2e6ac"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.bias torch.Size([1024, 1024])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([2304, 768])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.bias torch.Size([1024, 1024])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([2304, 768])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface gpt2 params\n",
        "```\n",
        "transformer.wte.weight torch.Size([50257, 768])\n",
        "transformer.wpe.weight torch.Size([1024, 768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "transformer.h.0.ln_1.bias torch.Size([768])\n",
        "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_2.weight torch.Size([768])\n",
        "transformer.h.0.ln_2.bias torch.Size([768])\n",
        "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.1.ln_1.weight torch.Size([768])\n",
        "transformer.h.1.ln_1.bias torch.Size([768])\n",
        "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.ln_f.weight torch.Size([768])\n",
        "transformer.ln_f.bias torch.Size([768])\n",
        "lm_head.weight torch.Size([50257, 768])\n",
        "```\n"
      ],
      "metadata": {
        "id": "Ya_VDLlaO-1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRDIiwKBTZpS",
        "outputId": "b36e946c-e390-4ac2-ddf2-b814ba8beb61"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(real_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE677-F8ezpq",
        "outputId": "f1611c04-0203-4060-f90a-95f904691a18"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate from model"
      ],
      "metadata": {
        "id": "V31yFhg-2GTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2.from_pretrained()"
      ],
      "metadata": {
        "id": "j_any7bue1vY"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnBUGXvD2C22",
        "outputId": "879e1a46-1a01-400f-c4e8-7bf2fa268cc1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): SelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "gHHLz1SX6Vmk"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hello World ! I'm LLM\"\n",
        "\n",
        "tokens = enc.encode(prompt)\n",
        "tokens = torch.tensor([tokens] ,dtype = torch.long)\n",
        "\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "id": "Rkw0pXvQ52kr"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI92nWNw54db",
        "outputId": "1d2324d4-adac-48a9-835b-ef708e2e1fd0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15496,  2159,  5145,   314,  1101, 27140,    44]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_max_tokens = 20\n",
        "\n",
        "while x.size(1) < new_max_tokens:\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(x)\n",
        "\n",
        "    probs = F.softmax(logits[:,-1,:],dim = -1)\n",
        "\n",
        "    new_token = torch.multinomial(probs,  num_samples=1)\n",
        "\n",
        "    x = torch.cat([x,new_token],dim=1)\n",
        "\n",
        "\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGiH2JG7cGF",
        "outputId": "c0ee8ec2-21c4-4f65-e0b9-33c7212404b7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15496,  2159,  5145,   314,  1101, 27140,    44,  1440,   329,   262,\n",
              "         23533,   198,  1450,   351,  1550,   510, 37131,   338,  2714,   423]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.view(-1).tolist()"
      ],
      "metadata": {
        "id": "yEqo4XPhCbTf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = enc.decode(x)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XSNZJVQqb3Tr",
        "outputId": "d81d1aad-830a-429d-cb77-93466d2a2e8d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello World ! I'm LLM four for the medicines\\n men with On up MacDonald's held have\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0xt7zKacJMA"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}