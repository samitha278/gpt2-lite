{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgLvcbKTgSgmGAVizQEFpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt2-lite/blob/main/repro_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bPIqVkE4774D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from transformers import GPT2LMHeadModel , pipeline , set_seed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "params_dict = model.state_dict()\n",
        "\n",
        "for k,v in params_dict.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYxb4Ymb9G94",
        "outputId": "8960dad6-e390-4021-834e-8de7e9726726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(220064)\n",
        "gen = pipeline('text-generation' , model = 'gpt2')\n",
        "prompt = \"Who is the first president is sri lanka?\"\n",
        "gen(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSVv2UGeBQMJ",
        "outputId": "67e1e557-5ca7-45d1-ea59-bb57081e35a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Who is the first president is sri lanka? sri sri lanka?\\n\\nPresident\\n\\npresident, sri, president, sri, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "zFv7ianNKwc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([\n",
        "    [[1,2,3],\n",
        "    [4,5,6]],\n",
        "\n",
        "    [[7,8,9],\n",
        "    [12,11,10]],\n",
        "\n",
        "    ])\n",
        "\n",
        "print(temp@temp.transpose(-2,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHK6wiS5Jts",
        "outputId": "983d6f6a-2015-492d-99f5-9bfbb7814978"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 14,  32],\n",
            "         [ 32,  77]],\n",
            "\n",
            "        [[194, 262],\n",
            "         [262, 365]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "DU4dcOWtEJQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "  batch_size = 32\n",
        "  block_size = 8\n",
        "  n_head = 4\n",
        "  n_layer = 6\n",
        "  n_embd = 128"
      ],
      "metadata": {
        "id": "kdwoieXV320Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "uvhc_246MjZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "    self.key = nn.Linear(n_embd,head_size)\n",
        "    self.query = nn.Linear(n_embd,head_size)\n",
        "    self.value = nn.Linear(n_embd,head_size)\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    weight = (query @ key.transpose(-1,-2)  ) * (self.head_size**-0.5)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim = -1)\n",
        "\n",
        "    value = self.value(x)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "aEd3CGid6Nk3"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    n_head = config.n_head\n",
        "    n_embd = config.n_embd\n",
        "\n",
        "\n",
        "    self.self_attns = nn.ModuleList([SelfAttentionHead(config)  for i in range(n_head)])\n",
        "    self.projection = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = torch.cat([sa(x) for sa in self.self_attns],dim=-1)\n",
        "    out = self.projection(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XQwRITlEkec"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = MultiHead(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wq2RceWBFB5",
        "outputId": "20aa1fde-899c-41d7-c29e-2cbd3fc3d8df"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHead(\n",
            "  (self_attns): ModuleList(\n",
            "    (0-3): 4 x SelfAttentionHead(\n",
            "      (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (value): Linear(in_features=128, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (projection): Linear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "self_attns.0.tril torch.Size([8, 8])\n",
            "self_attns.0.key.weight torch.Size([32, 128])\n",
            "self_attns.0.key.bias torch.Size([32])\n",
            "self_attns.0.query.weight torch.Size([32, 128])\n",
            "self_attns.0.query.bias torch.Size([32])\n",
            "self_attns.0.value.weight torch.Size([32, 128])\n",
            "self_attns.0.value.bias torch.Size([32])\n",
            "self_attns.1.tril torch.Size([8, 8])\n",
            "self_attns.1.key.weight torch.Size([32, 128])\n",
            "self_attns.1.key.bias torch.Size([32])\n",
            "self_attns.1.query.weight torch.Size([32, 128])\n",
            "self_attns.1.query.bias torch.Size([32])\n",
            "self_attns.1.value.weight torch.Size([32, 128])\n",
            "self_attns.1.value.bias torch.Size([32])\n",
            "self_attns.2.tril torch.Size([8, 8])\n",
            "self_attns.2.key.weight torch.Size([32, 128])\n",
            "self_attns.2.key.bias torch.Size([32])\n",
            "self_attns.2.query.weight torch.Size([32, 128])\n",
            "self_attns.2.query.bias torch.Size([32])\n",
            "self_attns.2.value.weight torch.Size([32, 128])\n",
            "self_attns.2.value.bias torch.Size([32])\n",
            "self_attns.3.tril torch.Size([8, 8])\n",
            "self_attns.3.key.weight torch.Size([32, 128])\n",
            "self_attns.3.key.bias torch.Size([32])\n",
            "self_attns.3.query.weight torch.Size([32, 128])\n",
            "self_attns.3.query.bias torch.Size([32])\n",
            "self_attns.3.value.weight torch.Size([32, 128])\n",
            "self_attns.3.value.bias torch.Size([32])\n",
            "projection.weight torch.Size([128, 128])\n",
            "projection.bias torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa = SelfAttentionHead(config)\n",
        "dic = sa.state_dict()\n",
        "\n",
        "print(sa)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCWfjaogGpO5",
        "outputId": "9cb9f588-9799-4414-f845-a3a9f9e5fef7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttentionHead(\n",
            "  (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (value): Linear(in_features=128, out_features=32, bias=True)\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.weight torch.Size([32, 128])\n",
            "key.bias torch.Size([32])\n",
            "query.weight torch.Size([32, 128])\n",
            "query.bias torch.Size([32])\n",
            "value.weight torch.Size([32, 128])\n",
            "value.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "sa(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOL314zJHtCy",
        "outputId": "bbf97d0a-555d-4fb9-bae1-b3131c258f75"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03vJtScFGxc",
        "outputId": "82b6c651-f291-4692-9b7e-06d16f56d7fc"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Head class + Multi Head class -> Single class"
      ],
      "metadata": {
        "id": "Sk9w2QM73mus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "out = torch.randint(5,(32,4,4,3))\n",
        "out[0][0],out[0][1],out[0][2],out[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGJdSFYwU6z",
        "outputId": "74643d90-a921-4454-bb04-6ee2cf521d5c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0, 4, 0],\n",
              "         [0, 1, 1],\n",
              "         [0, 0, 4],\n",
              "         [4, 2, 4]]),\n",
              " tensor([[4, 0, 4],\n",
              "         [0, 2, 4],\n",
              "         [2, 2, 4],\n",
              "         [3, 2, 2]]),\n",
              " tensor([[0, 2, 4],\n",
              "         [2, 3, 3],\n",
              "         [0, 1, 4],\n",
              "         [1, 4, 4]]),\n",
              " tensor([[3, 1, 1],\n",
              "         [3, 1, 4],\n",
              "         [3, 4, 4],\n",
              "         [3, 0, 3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,nh,T,C = out.shape\n",
        "out = out.permute(0,2,1,3)\n",
        "out = out.reshape(B,T,nh*C)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZaLcAMJym6T",
        "outputId": "5638682b-9cb0-4dbf-99b7-a904cf078300"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 4, 0, 0, 2, 4, 1, 0, 0, 4, 0, 0],\n",
              "        [0, 1, 1, 2, 3, 3, 1, 2, 2, 4, 3, 1],\n",
              "        [0, 0, 4, 0, 1, 4, 4, 2, 4, 2, 1, 2],\n",
              "        [4, 2, 4, 1, 4, 4, 0, 3, 3, 3, 1, 1],\n",
              "        [4, 0, 4, 3, 1, 1, 1, 0, 1, 2, 4, 1],\n",
              "        [0, 2, 4, 3, 1, 4, 2, 4, 4, 2, 2, 1],\n",
              "        [2, 2, 4, 3, 4, 4, 2, 4, 3, 2, 2, 1],\n",
              "        [3, 2, 2, 3, 0, 3, 0, 4, 1, 3, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = nn.ModuleList(nn.Linear(128,32) for _ in range(4))\n",
        "x = torch.randn((32,8,128))\n",
        "out1 = torch.stack([k(x) for k in key],dim=1)\n",
        "out2 = torch.stack([k(x) for k in key],dim=1)\n",
        "out = out1 @ out2.transpose(-1,-2)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFS9dObTXUo",
        "outputId": "e7f62a37-bf1a-463d-d987-36f2cdc1ab36"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "\n",
        "    self.key = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.query = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.value = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    key = torch.stack([k(x) for k in self.key],dim=1)\n",
        "    query = torch.stack([q(x) for q in self.query],dim=1)\n",
        "\n",
        "    weight = query @ key.transpose(-1,-2)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "    value = torch.stack([v(x) for v in self.value],dim=1)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    B,nh,T,C = out.shape\n",
        "    out = out.permute(0,2,1,3)\n",
        "    out = out.reshape(B,T,nh*C)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "IbRClTxwLGI7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = Attention(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGO-810DTkmz",
        "outputId": "f097056a-5560-40ec-b8dd-05091d21645b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention(\n",
            "  (key): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (query): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (value): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.0.weight torch.Size([32, 128])\n",
            "key.0.bias torch.Size([32])\n",
            "key.1.weight torch.Size([32, 128])\n",
            "key.1.bias torch.Size([32])\n",
            "key.2.weight torch.Size([32, 128])\n",
            "key.2.bias torch.Size([32])\n",
            "key.3.weight torch.Size([32, 128])\n",
            "key.3.bias torch.Size([32])\n",
            "query.0.weight torch.Size([32, 128])\n",
            "query.0.bias torch.Size([32])\n",
            "query.1.weight torch.Size([32, 128])\n",
            "query.1.bias torch.Size([32])\n",
            "query.2.weight torch.Size([32, 128])\n",
            "query.2.bias torch.Size([32])\n",
            "query.3.weight torch.Size([32, 128])\n",
            "query.3.bias torch.Size([32])\n",
            "value.0.weight torch.Size([32, 128])\n",
            "value.0.bias torch.Size([32])\n",
            "value.1.weight torch.Size([32, 128])\n",
            "value.1.bias torch.Size([32])\n",
            "value.2.weight torch.Size([32, 128])\n",
            "value.2.bias torch.Size([32])\n",
            "value.3.weight torch.Size([32, 128])\n",
            "value.3.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))"
      ],
      "metadata": {
        "id": "CinPQ96lYLSG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = h(x)"
      ],
      "metadata": {
        "id": "AyLUg3IyYYPv"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZ0GiTfa2Uo",
        "outputId": "39bad918-d468-48db-a208-acede3e64b04"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_BPwNCFy8Fs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}