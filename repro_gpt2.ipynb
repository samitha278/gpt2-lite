{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJlo/dksiC9AJ8cNxIidH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt2-lite/blob/main/repro_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bPIqVkE4774D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from transformers import GPT2LMHeadModel , pipeline , set_seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "params_dict = model.state_dict()\n",
        "\n",
        "for k,v in params_dict.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYxb4Ymb9G94",
        "outputId": "8960dad6-e390-4021-834e-8de7e9726726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_2.weight torch.Size([768])\n",
            "transformer.h.1.ln_2.bias torch.Size([768])\n",
            "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_1.weight torch.Size([768])\n",
            "transformer.h.2.ln_1.bias torch.Size([768])\n",
            "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.2.ln_2.weight torch.Size([768])\n",
            "transformer.h.2.ln_2.bias torch.Size([768])\n",
            "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_1.weight torch.Size([768])\n",
            "transformer.h.3.ln_1.bias torch.Size([768])\n",
            "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.3.ln_2.weight torch.Size([768])\n",
            "transformer.h.3.ln_2.bias torch.Size([768])\n",
            "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_1.weight torch.Size([768])\n",
            "transformer.h.4.ln_1.bias torch.Size([768])\n",
            "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.4.ln_2.weight torch.Size([768])\n",
            "transformer.h.4.ln_2.bias torch.Size([768])\n",
            "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_1.weight torch.Size([768])\n",
            "transformer.h.5.ln_1.bias torch.Size([768])\n",
            "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.5.ln_2.weight torch.Size([768])\n",
            "transformer.h.5.ln_2.bias torch.Size([768])\n",
            "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_1.weight torch.Size([768])\n",
            "transformer.h.6.ln_1.bias torch.Size([768])\n",
            "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.6.ln_2.weight torch.Size([768])\n",
            "transformer.h.6.ln_2.bias torch.Size([768])\n",
            "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_1.weight torch.Size([768])\n",
            "transformer.h.7.ln_1.bias torch.Size([768])\n",
            "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.7.ln_2.weight torch.Size([768])\n",
            "transformer.h.7.ln_2.bias torch.Size([768])\n",
            "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_1.weight torch.Size([768])\n",
            "transformer.h.8.ln_1.bias torch.Size([768])\n",
            "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.8.ln_2.weight torch.Size([768])\n",
            "transformer.h.8.ln_2.bias torch.Size([768])\n",
            "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_1.weight torch.Size([768])\n",
            "transformer.h.9.ln_1.bias torch.Size([768])\n",
            "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.9.ln_2.weight torch.Size([768])\n",
            "transformer.h.9.ln_2.bias torch.Size([768])\n",
            "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_1.weight torch.Size([768])\n",
            "transformer.h.10.ln_1.bias torch.Size([768])\n",
            "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.10.ln_2.weight torch.Size([768])\n",
            "transformer.h.10.ln_2.bias torch.Size([768])\n",
            "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_1.weight torch.Size([768])\n",
            "transformer.h.11.ln_1.bias torch.Size([768])\n",
            "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.ln_2.weight torch.Size([768])\n",
            "transformer.h.11.ln_2.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(220064)\n",
        "gen = pipeline('text-generation' , model = 'gpt2')\n",
        "prompt = \"Who is the first president is sri lanka?\"\n",
        "gen(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSVv2UGeBQMJ",
        "outputId": "67e1e557-5ca7-45d1-ea59-bb57081e35a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Who is the first president is sri lanka? sri sri lanka?\\n\\nPresident\\n\\npresident, sri, president, sri, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, president, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President, President,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "zFv7ianNKwc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([\n",
        "    [[1,2,3],\n",
        "    [4,5,6]],\n",
        "\n",
        "    [[7,8,9],\n",
        "    [12,11,10]],\n",
        "\n",
        "    ])\n",
        "\n",
        "print(temp@temp.transpose(-2,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHK6wiS5Jts",
        "outputId": "983d6f6a-2015-492d-99f5-9bfbb7814978"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 14,  32],\n",
            "         [ 32,  77]],\n",
            "\n",
            "        [[194, 262],\n",
            "         [262, 365]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "DU4dcOWtEJQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "  batch_size = 32\n",
        "  block_size = 8\n",
        "  n_head = 4\n",
        "  n_layer = 6\n",
        "  n_embd = 128"
      ],
      "metadata": {
        "id": "kdwoieXV320Y"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Head class + Multi Head class -> Single class"
      ],
      "metadata": {
        "id": "Sk9w2QM73mus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "\n",
        "    self.key = nn.Linear(n_embd,head_size)\n",
        "    self.query = nn.Linear(n_embd,head_size)\n",
        "    self.value = nn.Linear(n_embd,head_size)\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    weight = query @ key.transpose(-1,-2)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim = -1)\n",
        "\n",
        "    value = self.value(x)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "aEd3CGid6Nk3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    n_head = config.n_head\n",
        "    n_embd = config.n_embd\n",
        "\n",
        "\n",
        "    self.self_attns = nn.ModuleList([SelfAttentionHead(config)  for i in range(n_head)])\n",
        "    self.projection = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = torch.cat([sa(x) for sa in self.self_atts])\n",
        "    out = self.projection(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XQwRITlEkec"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = MultiHead(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wq2RceWBFB5",
        "outputId": "1f6bfbf4-f0a1-4d3d-a9ad-ac8392c21e4a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHead(\n",
            "  (self_attns): ModuleList(\n",
            "    (0-3): 4 x SelfAttentionHead(\n",
            "      (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (value): Linear(in_features=128, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (projection): Linear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "self_attns.0.tril torch.Size([8, 8])\n",
            "self_attns.0.key.weight torch.Size([32, 128])\n",
            "self_attns.0.key.bias torch.Size([32])\n",
            "self_attns.0.query.weight torch.Size([32, 128])\n",
            "self_attns.0.query.bias torch.Size([32])\n",
            "self_attns.0.value.weight torch.Size([32, 128])\n",
            "self_attns.0.value.bias torch.Size([32])\n",
            "self_attns.1.tril torch.Size([8, 8])\n",
            "self_attns.1.key.weight torch.Size([32, 128])\n",
            "self_attns.1.key.bias torch.Size([32])\n",
            "self_attns.1.query.weight torch.Size([32, 128])\n",
            "self_attns.1.query.bias torch.Size([32])\n",
            "self_attns.1.value.weight torch.Size([32, 128])\n",
            "self_attns.1.value.bias torch.Size([32])\n",
            "self_attns.2.tril torch.Size([8, 8])\n",
            "self_attns.2.key.weight torch.Size([32, 128])\n",
            "self_attns.2.key.bias torch.Size([32])\n",
            "self_attns.2.query.weight torch.Size([32, 128])\n",
            "self_attns.2.query.bias torch.Size([32])\n",
            "self_attns.2.value.weight torch.Size([32, 128])\n",
            "self_attns.2.value.bias torch.Size([32])\n",
            "self_attns.3.tril torch.Size([8, 8])\n",
            "self_attns.3.key.weight torch.Size([32, 128])\n",
            "self_attns.3.key.bias torch.Size([32])\n",
            "self_attns.3.query.weight torch.Size([32, 128])\n",
            "self_attns.3.query.bias torch.Size([32])\n",
            "self_attns.3.value.weight torch.Size([32, 128])\n",
            "self_attns.3.value.bias torch.Size([32])\n",
            "projection.weight torch.Size([128, 128])\n",
            "projection.bias torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa = SelfAttentionHead(config)\n",
        "dic = sa.state_dict()\n",
        "\n",
        "print(sa)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCWfjaogGpO5",
        "outputId": "952fde22-4394-4001-e581-55bb7d8fb25a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttentionHead(\n",
            "  (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (value): Linear(in_features=128, out_features=32, bias=True)\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.weight torch.Size([32, 128])\n",
            "key.bias torch.Size([32])\n",
            "query.weight torch.Size([32, 128])\n",
            "query.bias torch.Size([32])\n",
            "value.weight torch.Size([32, 128])\n",
            "value.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "sa(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOL314zJHtCy",
        "outputId": "383fcdcc-8511-4876-8fe7-1ee3ab573f43"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f03vJtScFGxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}