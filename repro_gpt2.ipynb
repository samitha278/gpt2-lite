{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJKcxiam3pXy6VsErgm4tY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt2-lite/blob/main/repro_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "bPIqVkE4774D"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from transformers import GPT2LMHeadModel , pipeline , set_seed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "params_dict = model.state_dict()"
      ],
      "metadata": {
        "id": "wYxb4Ymb9G94"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_d = {}\n",
        "for k,v, in params_dict.items():\n",
        "   real_d[k] = v.shape"
      ],
      "metadata": {
        "id": "WCaGOmn4T-Ap"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (k, v) in enumerate(params_dict.items()):\n",
        "    if i < 20 or i > len(params_dict) - 5:\n",
        "        print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0cM_AfU_3cZ",
        "outputId": "4cd5e132-d8af-41a9-d860-28e340387e09"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(220064)\n",
        "gen = pipeline('text-generation' , model = 'gpt2')\n",
        "prompt = \"Who is the first president is sri lanka?\"\n",
        "gen(prompt,max_new_tokens= 10 , max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSVv2UGeBQMJ",
        "outputId": "fc74a76a-e2cc-4880-b8c9-e03536f7930e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=10) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Who is the first president is sri lanka? sri sri lanka?\\n\\nPresident'}]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "zFv7ianNKwc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([\n",
        "    [[1,2,3],\n",
        "    [4,5,6]],\n",
        "\n",
        "    [[7,8,9],\n",
        "    [12,11,10]],\n",
        "\n",
        "    ])\n",
        "\n",
        "print(temp@temp.transpose(-2,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHK6wiS5Jts",
        "outputId": "58c71969-ceb1-47db-ffe2-20173ac45862"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 14,  32],\n",
            "         [ 32,  77]],\n",
            "\n",
            "        [[194, 262],\n",
            "         [262, 365]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "DU4dcOWtEJQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "  batch_size = 32\n",
        "  block_size = 8\n",
        "  n_head = 4\n",
        "  n_layer = 6\n",
        "  n_embd = 128"
      ],
      "metadata": {
        "id": "kdwoieXV320Y"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "uvhc_246MjZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "    self.key = nn.Linear(n_embd,head_size)\n",
        "    self.query = nn.Linear(n_embd,head_size)\n",
        "    self.value = nn.Linear(n_embd,head_size)\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    key = self.key(x)\n",
        "    query = self.query(x)\n",
        "\n",
        "    weight = (query @ key.transpose(-1,-2)  ) * (self.head_size**-0.5)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim = -1)\n",
        "\n",
        "    value = self.value(x)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "aEd3CGid6Nk3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    n_head = config.n_head\n",
        "    n_embd = config.n_embd\n",
        "\n",
        "\n",
        "    self.self_attns = nn.ModuleList([SelfAttentionHead(config)  for i in range(n_head)])\n",
        "    self.projection = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = torch.cat([sa(x) for sa in self.self_attns],dim=-1)\n",
        "    out = self.projection(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XQwRITlEkec"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = MultiHead(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wq2RceWBFB5",
        "outputId": "8ab065e4-e96a-4cd2-e017-a8e9214961c3"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHead(\n",
            "  (self_attns): ModuleList(\n",
            "    (0-3): 4 x SelfAttentionHead(\n",
            "      (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "      (value): Linear(in_features=128, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (projection): Linear(in_features=128, out_features=128, bias=True)\n",
            ")\n",
            "self_attns.0.tril torch.Size([8, 8])\n",
            "self_attns.0.key.weight torch.Size([32, 128])\n",
            "self_attns.0.key.bias torch.Size([32])\n",
            "self_attns.0.query.weight torch.Size([32, 128])\n",
            "self_attns.0.query.bias torch.Size([32])\n",
            "self_attns.0.value.weight torch.Size([32, 128])\n",
            "self_attns.0.value.bias torch.Size([32])\n",
            "self_attns.1.tril torch.Size([8, 8])\n",
            "self_attns.1.key.weight torch.Size([32, 128])\n",
            "self_attns.1.key.bias torch.Size([32])\n",
            "self_attns.1.query.weight torch.Size([32, 128])\n",
            "self_attns.1.query.bias torch.Size([32])\n",
            "self_attns.1.value.weight torch.Size([32, 128])\n",
            "self_attns.1.value.bias torch.Size([32])\n",
            "self_attns.2.tril torch.Size([8, 8])\n",
            "self_attns.2.key.weight torch.Size([32, 128])\n",
            "self_attns.2.key.bias torch.Size([32])\n",
            "self_attns.2.query.weight torch.Size([32, 128])\n",
            "self_attns.2.query.bias torch.Size([32])\n",
            "self_attns.2.value.weight torch.Size([32, 128])\n",
            "self_attns.2.value.bias torch.Size([32])\n",
            "self_attns.3.tril torch.Size([8, 8])\n",
            "self_attns.3.key.weight torch.Size([32, 128])\n",
            "self_attns.3.key.bias torch.Size([32])\n",
            "self_attns.3.query.weight torch.Size([32, 128])\n",
            "self_attns.3.query.bias torch.Size([32])\n",
            "self_attns.3.value.weight torch.Size([32, 128])\n",
            "self_attns.3.value.bias torch.Size([32])\n",
            "projection.weight torch.Size([128, 128])\n",
            "projection.bias torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa = SelfAttentionHead(config)\n",
        "dic = sa.state_dict()\n",
        "\n",
        "print(sa)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCWfjaogGpO5",
        "outputId": "753b1c49-56ca-44a4-f9ca-a4bfebe32edf"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttentionHead(\n",
            "  (key): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (query): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (value): Linear(in_features=128, out_features=32, bias=True)\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.weight torch.Size([32, 128])\n",
            "key.bias torch.Size([32])\n",
            "query.weight torch.Size([32, 128])\n",
            "query.bias torch.Size([32])\n",
            "value.weight torch.Size([32, 128])\n",
            "value.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "sa(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOL314zJHtCy",
        "outputId": "b2ae7d79-7116-4982-d40f-1ee108ec4528"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03vJtScFGxc",
        "outputId": "480433a9-4049-4d5f-b420-8e5d5d76014a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention Head class + Multi Head class -> Single class"
      ],
      "metadata": {
        "id": "Sk9w2QM73mus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "out = torch.randint(5,(32,4,4,3))\n",
        "out[0][0],out[0][1],out[0][2],out[0][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzGJdSFYwU6z",
        "outputId": "2b573014-86d8-4735-a287-e3446857cd72"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0, 4, 0],\n",
              "         [0, 1, 1],\n",
              "         [0, 0, 4],\n",
              "         [4, 2, 4]]),\n",
              " tensor([[4, 0, 4],\n",
              "         [0, 2, 4],\n",
              "         [2, 2, 4],\n",
              "         [3, 2, 2]]),\n",
              " tensor([[0, 2, 4],\n",
              "         [2, 3, 3],\n",
              "         [0, 1, 4],\n",
              "         [1, 4, 4]]),\n",
              " tensor([[3, 1, 1],\n",
              "         [3, 1, 4],\n",
              "         [3, 4, 4],\n",
              "         [3, 0, 3]]))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,nh,T,C = out.shape\n",
        "out = out.permute(0,2,1,3)\n",
        "out = out.reshape(B,T,nh*C)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZaLcAMJym6T",
        "outputId": "11efcb54-e569-44ce-9d4b-d039069339e3"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 4, 0, 4, 0, 4, 0, 2, 4, 3, 1, 1],\n",
              "        [0, 1, 1, 0, 2, 4, 2, 3, 3, 3, 1, 4],\n",
              "        [0, 0, 4, 2, 2, 4, 0, 1, 4, 3, 4, 4],\n",
              "        [4, 2, 4, 3, 2, 2, 1, 4, 4, 3, 0, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = nn.ModuleList(nn.Linear(128,32) for _ in range(4))\n",
        "x = torch.randn((32,8,128))\n",
        "out1 = torch.stack([k(x) for k in key],dim=1)\n",
        "out2 = torch.stack([k(x) for k in key],dim=1)\n",
        "out = out1 @ out2.transpose(-1,-2)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFS9dObTXUo",
        "outputId": "e05e0788-e4db-44dd-c1d4-93e949dd98d0"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    block_size = config.block_size\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    head_size = n_embd // n_head\n",
        "    self.head_size = head_size\n",
        "\n",
        "\n",
        "\n",
        "    self.key = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.query = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "    self.value = nn.ModuleList(nn.Linear(n_embd,head_size) for _ in range(n_head))\n",
        "\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    key = torch.stack([k(x) for k in self.key],dim=1)\n",
        "    query = torch.stack([q(x) for q in self.query],dim=1)\n",
        "\n",
        "    weight = query @ key.transpose(-1,-2)\n",
        "    weight = weight.masked_fill(self.tril[:]==0,float('-inf'))\n",
        "    weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "    value = torch.stack([v(x) for v in self.value],dim=1)\n",
        "\n",
        "    out = weight @ value\n",
        "\n",
        "    B,nh,T,C = out.shape\n",
        "    out = out.permute(0,2,1,3)\n",
        "    out = out.reshape(B,T,nh*C)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "IbRClTxwLGI7"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = Attention(config)\n",
        "dic = h.state_dict()\n",
        "print(h)\n",
        "for k,v, in dic.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGO-810DTkmz",
        "outputId": "7bac894f-5f48-4b4e-f3a8-4081ac6939c8"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention(\n",
            "  (key): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (query): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            "  (value): ModuleList(\n",
            "    (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
            "  )\n",
            ")\n",
            "tril torch.Size([8, 8])\n",
            "key.0.weight torch.Size([32, 128])\n",
            "key.0.bias torch.Size([32])\n",
            "key.1.weight torch.Size([32, 128])\n",
            "key.1.bias torch.Size([32])\n",
            "key.2.weight torch.Size([32, 128])\n",
            "key.2.bias torch.Size([32])\n",
            "key.3.weight torch.Size([32, 128])\n",
            "key.3.bias torch.Size([32])\n",
            "query.0.weight torch.Size([32, 128])\n",
            "query.0.bias torch.Size([32])\n",
            "query.1.weight torch.Size([32, 128])\n",
            "query.1.bias torch.Size([32])\n",
            "query.2.weight torch.Size([32, 128])\n",
            "query.2.bias torch.Size([32])\n",
            "query.3.weight torch.Size([32, 128])\n",
            "query.3.bias torch.Size([32])\n",
            "value.0.weight torch.Size([32, 128])\n",
            "value.0.bias torch.Size([32])\n",
            "value.1.weight torch.Size([32, 128])\n",
            "value.1.bias torch.Size([32])\n",
            "value.2.weight torch.Size([32, 128])\n",
            "value.2.bias torch.Size([32])\n",
            "value.3.weight torch.Size([32, 128])\n",
            "value.3.bias torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))"
      ],
      "metadata": {
        "id": "CinPQ96lYLSG"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = h(x)"
      ],
      "metadata": {
        "id": "AyLUg3IyYYPv"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZ0GiTfa2Uo",
        "outputId": "0525fb87-675a-413d-e3c6-6f7af66ec35b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention version"
      ],
      "metadata": {
        "id": "OYvO_Nk631cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        block_size = config.block_size\n",
        "\n",
        "        self.n_head = n_embd = config.n_head\n",
        "        self.n_embd = n_head = config.n_embd\n",
        "\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)     # fan out : n_head * 3 * head_size\n",
        "\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # C = n_embd = n_head * head_size\n",
        "\n",
        "        qkv = self.c_attn(x)    # B,T, 3*n_embd\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)    # each : B,T, n_head * head_size\n",
        "\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)         # B, n_head, T, T\n",
        "        att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v        # B, n_head, T, head_size\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)   # B, T , n_embd   (n_embd = n_head * head_size)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "o_BPwNCFy8Fs"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "h = Attention(config)\n",
        "# dic = h.state_dict()\n",
        "# print(h)\n",
        "# for k,v, in dic.items():\n",
        "#   print(k,v.shape)"
      ],
      "metadata": {
        "id": "e_ShacqE4YFK"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((32,8,128))\n",
        "h(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUMU6hn34HQj",
        "outputId": "cc16cbdd-ce59-49e9-927c-c7710991b013"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch pretrained params from huggingface gpt-2 model"
      ],
      "metadata": {
        "id": "TYoo7YQULH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "@dataclass\n",
        "class GPT2Config:\n",
        "    block_size : int = 1024\n",
        "    vocab_size : int = 50257\n",
        "    n_layer : int = 12\n",
        "    n_head : int = 12\n",
        "    n_embd : int = 768\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size,config.n_embd),\n",
        "\n",
        "            h = nn.ModuleList([Block(config) for i in range(config.n_layer)]),\n",
        "\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd,config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x,targets= None):\n",
        "\n",
        "        tx = self.transformer.wte(x)       #token embedding\n",
        "        px = self.transformer.wpe(torch.arnage(self.config.block_size,device=device)) #positional embedding\n",
        "\n",
        "        x = tx+px     # add both\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits\n",
        "\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1,self.config.n_embd),targets.view(-1))\n",
        "            return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type='gpt2'):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        assert model_type == 'gpt2'\n",
        "\n",
        "        config_args = dict(n_layer=12, n_head=12, n_embd=768, vocab_size=50257, block_size=1024)\n",
        "        config = GPT2Config(**config_args)\n",
        "        model = GPT2(config)\n",
        "\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias')]\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith(('.attn.masked_bias', '.attn.bias'))]\n",
        "\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys)\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,idx,max_token):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.attn(self.ln1(x)) + x\n",
        "        x = self.mlp(self.ln2(x)) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.c_fc = nn.Linear(config.n_embd,4*config.n_embd)\n",
        "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.c_fc(x)\n",
        "        x = nn.GELU(x)\n",
        "        x = self.c_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        block_size = config.block_size\n",
        "\n",
        "        self.n_head = n_head = config.n_head\n",
        "        self.n_embd = n_embd = config.n_embd\n",
        "\n",
        "\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)     # fan out : n_head * 3 * head_size\n",
        "\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # C = n_embd = n_head * head_size\n",
        "\n",
        "        qkv = self.c_attn(x)    # B,T, 3*n_embd\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)    # each : B,T, n_head * head_size\n",
        "\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)         # B, n_head, T, T\n",
        "        att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v        # B, n_head, T, head_size\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)   # B, T , n_embd   (n_embd = n_head * head_size)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0PrZxQLYOLia"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config()\n",
        "model = GPT2(config)\n",
        "dic = model.state_dict()\n",
        "\n",
        "\n",
        "new_d = {}\n",
        "for k,v, in dic.items():\n",
        "   new_d[k] = v.shape"
      ],
      "metadata": {
        "id": "TCvhKDb0LPGa"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (k, v) in enumerate(dic.items()):\n",
        "    if i < 20 or i > len(dic) - 5:\n",
        "        print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5uZcEI8eofm",
        "outputId": "fc3a7ec9-bb7f-4cbe-f92b-8f6bdae47a6b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight torch.Size([50257, 768])\n",
            "transformer.wpe.weight torch.Size([1024, 768])\n",
            "transformer.h.0.ln_1.weight torch.Size([768])\n",
            "transformer.h.0.ln_1.bias torch.Size([768])\n",
            "transformer.h.0.attn.bias torch.Size([1024, 1024])\n",
            "transformer.h.0.attn.c_attn.weight torch.Size([2304, 768])\n",
            "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
            "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
            "transformer.h.0.ln_2.weight torch.Size([768])\n",
            "transformer.h.0.ln_2.bias torch.Size([768])\n",
            "transformer.h.0.mlp.c_fc.weight torch.Size([3072, 768])\n",
            "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
            "transformer.h.0.mlp.c_proj.weight torch.Size([768, 3072])\n",
            "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.h.1.ln_1.weight torch.Size([768])\n",
            "transformer.h.1.ln_1.bias torch.Size([768])\n",
            "transformer.h.1.attn.bias torch.Size([1024, 1024])\n",
            "transformer.h.1.attn.c_attn.weight torch.Size([2304, 768])\n",
            "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
            "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
            "transformer.ln_f.weight torch.Size([768])\n",
            "transformer.ln_f.bias torch.Size([768])\n",
            "lm_head.weight torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface gpt2 params\n",
        "```\n",
        "transformer.wte.weight torch.Size([50257, 768])\n",
        "transformer.wpe.weight torch.Size([1024, 768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "transformer.h.0.ln_1.bias torch.Size([768])\n",
        "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_2.weight torch.Size([768])\n",
        "transformer.h.0.ln_2.bias torch.Size([768])\n",
        "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.1.ln_1.weight torch.Size([768])\n",
        "transformer.h.1.ln_1.bias torch.Size([768])\n",
        "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.ln_f.weight torch.Size([768])\n",
        "transformer.ln_f.bias torch.Size([768])\n",
        "lm_head.weight torch.Size([50257, 768])\n",
        "```\n"
      ],
      "metadata": {
        "id": "Ya_VDLlaO-1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRDIiwKBTZpS",
        "outputId": "b7d70b51-eb94-4a48-da75-5e2625c2d04d"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(real_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE677-F8ezpq",
        "outputId": "67cdceea-4d38-4dcf-bc5b-7d72241f7f91"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2.from_pretrained()\n",
        "print(\"worked\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_any7bue1vY",
        "outputId": "6a51a741-b7be-41af-a6cf-a04ba5a5954e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APmwZC4YmP2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}