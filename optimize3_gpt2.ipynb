{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsDkqIre1CeG2i02kQDDa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt2-lite/blob/main/optimize3_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0ycgQlibUyD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gdown\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data file doenload\n",
        "\n",
        "file_id = \"1ia6z4itw7WJWpnoTohURX6Lm-AnZmVZz\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"input.txt\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "2fFwz3Kic-Rk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7105a0e3-9f15-485e-bac4-365c756dd9df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ia6z4itw7WJWpnoTohURX6Lm-AnZmVZz\n",
            "To: /content/input.txt\n",
            "100%|██████████| 1.12M/1.12M [00:00<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'input.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Model"
      ],
      "metadata": {
        "id": "5Qo3_bM9c-5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPT2Config:\n",
        "    block_size : int = 1024\n",
        "    vocab_size : int = 50257\n",
        "    n_layer : int = 12\n",
        "    n_head : int = 12\n",
        "    n_embd : int = 768\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size,config.n_embd),\n",
        "\n",
        "            h = nn.ModuleList([Block(config) for i in range(config.n_layer)]),\n",
        "\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd,config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "        #weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        #Save ~38M parameters\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def _init_weights(self,module):\n",
        "\n",
        "        if isinstance(module,nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module,'FLAG'):\n",
        "                std *= (2*self.config.n_layer) ** -0.5           #scaledown std\n",
        "            torch.nn.init.normal_(module.weight,mean=0.0,std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module,nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x,targets= None):\n",
        "\n",
        "        B,T = x.shape\n",
        "        assert T<= self.config.block_size   # positional embd table max size = block_size\n",
        "        tx = self.transformer.wte(x)       #token embedding\n",
        "        px = self.transformer.wpe(torch.arange(0,T,self.config.block_size,device=device)) #positional embedding\n",
        "\n",
        "        x = tx+px     # add both\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(B*T,-1) ,targets.view(-1))\n",
        "            return logits,loss\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type='gpt2'):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        assert model_type == 'gpt2'\n",
        "\n",
        "        config_args = dict(n_layer=12, n_head=12, n_embd=768, vocab_size=50257, block_size=1024)\n",
        "        config = GPT2Config(**config_args)\n",
        "        model = GPT2(config)\n",
        "\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias')]\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith(('.attn.masked_bias', '.attn.bias'))]\n",
        "\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(sd_keys_hf) == len(sd_keys)\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.attn(self.ln_1(x)) + x\n",
        "        x = self.mlp(self.ln_2(x)) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.c_fc = nn.Linear(config.n_embd,4*config.n_embd)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
        "        self.c_proj.FLAG = 1\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        block_size = config.block_size\n",
        "\n",
        "        self.n_head = n_head = config.n_head\n",
        "        self.n_embd = n_embd = config.n_embd\n",
        "\n",
        "\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)     # fan out : n_head * 3 * head_size\n",
        "\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.c_proj.FLAG = 1\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # C = n_embd = n_head * head_size\n",
        "\n",
        "        qkv = self.c_attn(x)    # B,T, 3*n_embd\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)    # each : B,T, n_head * head_size\n",
        "\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)    # \"\"\n",
        "\n",
        "        # att = (q @ k.transpose(-2, -1)) * (self.head_size**-0.5)         # B, n_head, T, T\n",
        "        # att = att.masked_fill(self.bias[:T,:T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "\n",
        "        # y = att @ v        # B, n_head, T, head_size\n",
        "\n",
        "\n",
        "        # Flash Attention\n",
        "        y = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n",
        "\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)   # B, T , n_embd   (n_embd = n_head * head_size)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "class DataLoader():\n",
        "\n",
        "  def __init__(self,B,T):\n",
        "\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    self.tokens = torch.tensor(enc.encode(text))\n",
        "\n",
        "    print(f'1 epoch size: {len(self.tokens//B*T)}')\n",
        "\n",
        "    self.count = 0\n",
        "\n",
        "\n",
        "\n",
        "  def get_batch(self):\n",
        "\n",
        "    B,T = self.B , self.T\n",
        "\n",
        "    temp = self.tokens[self.count:self.count+B*T+1]\n",
        "\n",
        "    x = temp[:-1].view(B,T)   #inputs\n",
        "    y = temp[1:].view(B,T)    #targets\n",
        "\n",
        "    self.count += B*T\n",
        "\n",
        "    # Reset\n",
        "    if (self.count+B*T+1) > len(self.tokens):\n",
        "      self.count = 0\n",
        "\n",
        "    return x,y\n"
      ],
      "metadata": {
        "id": "CkCfDtXmdkTn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "9URVKdESduoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 4\n",
        "T = 1024\n",
        "\n",
        "\n",
        "torch.manual_seed(278)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(278)\n",
        "\n",
        "\n",
        "model = GPT2(GPT2Config(vocab_size = 50304))\n",
        "model = model.to(device)\n",
        "model = torch.compile(model)    # compile model into optimize form\n",
        "\n",
        "\n",
        "data = DataLoader(B,T)\n",
        "\n",
        "# _____________________________________________________________________________\n",
        "\n",
        "# Learning Rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "\n",
        "max_iter = 1000\n",
        "warmup_steps = max_iter * 0.05\n",
        "\n",
        "def next_lr(i):\n",
        "  # warmup stage : linear\n",
        "  if i < warmup_steps :\n",
        "    return (max_lr/warmup_steps) * (i+1)\n",
        "\n",
        "  if i > max_iter:\n",
        "    return min_lr\n",
        "\n",
        "  # cosine dacay\n",
        "  decay_ratio = (i-warmup_steps) / (max_iter-warmup_steps)\n",
        "  assert 0<= decay_ratio <=1\n",
        "  c = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "  return min_lr + c * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "# _____________________________________________________________________________\n",
        "\n",
        "\n",
        "losses = torch.zeros((max_iter,))\n",
        "norms = torch.zeros((max_iter,))\n",
        "lrs = torch.zeros((max_iter,))\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 6e-4,betas = (0.9,0.95),eps = 1e-8)\n",
        "#Gradient Scalar\n",
        "scaler = torch.amp.GradScaler(device)     # Prevents gradient underflow when using FP16\n",
        "\n",
        "#optimize\n",
        "for i in range(max_iter):\n",
        "\n",
        "  t0 = time.time()   # time start\n",
        "\n",
        "  xb , yb = data.get_batch()\n",
        "  xb , yb = xb.to(device),yb.to(device)\n",
        "\n",
        "  #AMP\n",
        "  with torch.autocast(device_type=device, dtype=torch.float16):   # FP16\n",
        "    logits , loss = model(xb,yb)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  scaler.scale(loss).backward()     # multiplies loss by a scale factor\n",
        "\n",
        "  norm = nn.utils.clip_grad_norm_(model.parameters(),1.0)    # inplace gradient clipping\n",
        "\n",
        "  # find and set learning rate\n",
        "  lr = next_lr(i)\n",
        "\n",
        "  # update optimizer this new lr\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "  scaler.step(optimizer)            # unscales gradients then call optimizer step\n",
        "  scaler.update()                   # adjusts the scale factor automatically each iteration\n",
        "\n",
        "\n",
        "  torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "\n",
        "  t1 = time.time()   # time end\n",
        "  t = (t1 - t0)*1000 # ms\n",
        "\n",
        "  losses[i] = loss.item()\n",
        "  norms[i] = norm.item()\n",
        "  lrs[i] = lr\n",
        "\n",
        "  if i%100==0 : print(f'{i}/{max_iter}  {loss.item():.4f}  {t:.4f} ms  norm:{norm.item():.4f}  lr:{lr:.4f}')"
      ],
      "metadata": {
        "id": "nZmWNI7ZdtwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b75f91d0-3e4c-4935-9f44-18698ec1f89b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch size: 338025\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-850320609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#Gradient Scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Prevents gradient underflow when using FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Losses"
      ],
      "metadata": {
        "id": "zlphwojWTm5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ppq-P2ytTfpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Norms"
      ],
      "metadata": {
        "id": "IOGncMPUTvo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(norms)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O33_jek4Ti1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rates"
      ],
      "metadata": {
        "id": "z801ThwtTzZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(lrs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sdqhgbYUTlfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}